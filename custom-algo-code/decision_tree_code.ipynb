{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import pydotplus\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_occurring_feature(Y):\n",
    "    counter = Counter(Y)\n",
    "    k, v = counter.most_common(1)[0]\n",
    "    return k\n",
    "    \n",
    "    \n",
    "\"\"\" functions to calculate impurity and gain \"\"\"\n",
    "\n",
    "# i) method - information gain\n",
    "def entropy(Y):\n",
    "        \"\"\" Y :  List(data) representing classes.\n",
    "            entropy = -∑pi*log(pi) \n",
    "        \"\"\"\n",
    "        frequency = Counter(Y)\n",
    "        entropy = 0\n",
    "        total = len(Y)\n",
    "        for i in frequency:\n",
    "            p = frequency[i]/total\n",
    "            entropy -= (p)*math.log2(p)\n",
    "        return entropy\n",
    "    \n",
    "    \n",
    "def gain_ratio(previous_classes, current_classes):\n",
    "        \"\"\" previous_classes (list(int)): represents classes before split.\n",
    "            current_classes (list(list(int): A list of lists of classes after split according to current feature.\n",
    "        \"\"\"\n",
    "        original_impurity = entropy(previous_classes) #-----> original_impurity represents entropy before splitting\n",
    "        current_impurity = 0  #-----------------------------> entropy after splitting upon the selected feature\n",
    "        split_info = 0  #-----------------------------------> penalty\n",
    "        previous_size = len(previous_classes)\n",
    "        for class_i in current_classes:\n",
    "            current_size = len(class_i)\n",
    "            current_impurity += (current_size/previous_size)*entropy(class_i)\n",
    "            split_info += (-current_size/previous_size)*math.log2(current_size/previous_size)\n",
    "\n",
    "        # edge case to handle 0 division error\n",
    "        if split_info == 0 :\n",
    "            return math.inf\n",
    "\n",
    "        info_gain = original_impurity - current_impurity\n",
    "        return (info_gain / split_info) #-------------------> gain ratio\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "# ii) method - gini index\n",
    "\n",
    "def gini_index(Y):\n",
    "    prob_sum = 0 #------------------------------------------> variable to store ∑pi^2 value\n",
    "    freq_map = Counter(Y)\n",
    "    n = len(Y) #--------------------------------------------> count of all classes\n",
    "    for class_i in freq_map:\n",
    "        prob_sum += (freq_map[class_i]/n)**2 #--------------> pi^2 => (count of ith class/count of all classes)^2\n",
    "\n",
    "    return 1 - prob_sum # ----------------------------------> Gini_index = 1 − ∑pi^2\n",
    "    \n",
    "    \n",
    "def gini_gain(previous_classes, current_classes):\n",
    "    \"\"\" previous_classes (list(data)): Vector of classes.\n",
    "        current_classes (list(list(data): A list of lists of classes divided by the current feature.\n",
    "    \"\"\"\n",
    "    original_impurity = gini_index(previous_classes)\n",
    "    current_impurity = 0 #---------------------------------> stores weighted sum of gini impurities of every split sections\n",
    "    previous_len = len(previous_classes)\n",
    "    \n",
    "    # edge case - if one of the splitted class is empty\n",
    "    if len(current_classes[0]) == 0 or len(current_classes[1]) == 0:\n",
    "        return 0\n",
    "\n",
    "    for current_class in current_classes:\n",
    "        weightage = len(current_class)/previous_len\n",
    "        current_impurity += weightage * gini_index(current_class)\n",
    "\n",
    "    return original_impurity - current_impurity #----------> total gain\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" split related funtions\"\"\"\n",
    "\n",
    "# function to split the dataset in 2 parts according to feature values \n",
    "def split_by_feature(X,Y, feature, split_val):\n",
    "    x_slices = [[],[]] # ----------------------------------> stores left split and right split of X\n",
    "    y_slices = [[],[]] # ----------------------------------> stores left split and right split of Y\n",
    "    \n",
    "    for row in range(len(X)):\n",
    "        if X[row, feature]<= split_val:\n",
    "            col = 0 #--------------------------------------> left split index\n",
    "        else:\n",
    "            col = 1 #--------------------------------------> right split index\n",
    "        x_slices[col].append(X[row,:])\n",
    "        y_slices[col].append(Y[row])\n",
    "\n",
    "    return x_slices, y_slices\n",
    "\n",
    "\n",
    "# function to search for best split value using split_by_feature function\n",
    "def best_split(X, Y, feature, gini):\n",
    "    \"\"\" feature : column index of X which represents the splitting feature\n",
    "        gini : boolean value to decide the gain method (true:gini_index, false:entropy)\n",
    "    \"\"\"\n",
    "    values = list(set(X[:, feature])) # -------------------> set of unique feature values\n",
    "    max_gain = -1\n",
    "    best_x_split = None # ---------------------------------> stores the best split of X depending on max_gain\n",
    "    best_y_split = None # ---------------------------------> stores the best split of Y depending on max_gain\n",
    "    best_split_val = -1 #----------------------------------> stores the best feature value which yields best gain\n",
    "    for i in range(1,len(values)):\n",
    "        val = (values[i-1]+values[i])/2 # --------------------> split value\n",
    "        x_split, y_split = split_by_feature(X, Y, feature, val)\n",
    "        if gini:\n",
    "            gain = gini_gain(Y, y_split)\n",
    "        else:\n",
    "            gain = gain_ratio(Y, y_split)\n",
    "        if gain>max_gain:\n",
    "            max_gain = gain\n",
    "            best_x_split = x_split\n",
    "            best_y_split = y_split\n",
    "            best_split_val = val\n",
    "    return best_x_split, best_y_split, max_gain, best_split_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, deciding_feature=None, split_val=None, class_mark=None, entropy=0):\n",
    "        self.left = None #---------------------------------> left child initialized as None\n",
    "        self.right = None #--------------------------------> right child initialized as None\n",
    "        self.deciding_feature = deciding_feature  #--------> feature with max gain\n",
    "        self.split_val = split_val\n",
    "        self.deciding_func = lambda x: (x[deciding_feature]<=split_val)\n",
    "        self.class_mark = class_mark  #--------------------> holds output class value for leaf node otherwise None\n",
    "        self.entropy = entropy #---------------------------> impurity amongst output\n",
    "    \n",
    "    def decide(self, x):\n",
    "        if self.class_mark is not None: #------------------> only leaf contains a class mark\n",
    "            return self.class_mark \n",
    "        if self.deciding_func(x):\n",
    "            return self.left.decide(x)\n",
    "        return self.right.decide(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the class for decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self,depth_limit = math.inf):\n",
    "        self.root = None\n",
    "        self.depth_limit = depth_limit\n",
    "        \n",
    "    \n",
    "    def fit(self, X,Y, feature_names= None, metric=\"gain_ratio\"):\n",
    "        \"\"\" function to build decision tree according to given training data\n",
    "            and store the root Node\n",
    "        \"\"\"\n",
    "        # creating feature names if not passed\n",
    "        if feature_names is None:\n",
    "            feature_names = [\"X[\"+str(i)+\"]\" for i in range(len(X[0]))] #-----> storing feature name as X[i]\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        feature_indexes = [i for i in range(len(X[0]))] #---------------------> using numbers in actual functions is easier\n",
    "        \n",
    "        self.classes = list(set(Y))\n",
    "        \n",
    "        self.gini = False #---------------------------------------------------> represents gain calculation method (True: gini_index, False: info gain)\n",
    "        if (metric.lower()==\"gini_index\"):\n",
    "            self.gini = True\n",
    "            \n",
    "        self.root = self.generate(X,Y, feature_indexes)\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "    def generate(self, X,Y,features, level = 0):\n",
    "        print(\"Level\",level)\n",
    "        \n",
    "        # Edge Case 1 - pure output set\n",
    "        if len(set(Y)) == 1:\n",
    "            output = Y[0]\n",
    "            print(\"Count of\",Y[0],\"=\",len(Y))\n",
    "            if self.gini:\n",
    "                print(\"Current gini impurity is =  0.0\")\n",
    "            else:\n",
    "                print(\"Current Entropy is =  0.0\")\n",
    "            print(\"Reached leaf Node\\n\")\n",
    "            return Node(class_mark=output)\n",
    "        \n",
    "        # common part\n",
    "        # print counts of classes\n",
    "        freq_map = Counter(Y)\n",
    "        for i in freq_map:\n",
    "            print(\"Count of\",i,\"=\",freq_map[i])\n",
    "        # print current impurity\n",
    "        impurity = 0\n",
    "        if self.gini:\n",
    "            impurity = gini_index(Y)\n",
    "            print(\"Current gini impurity is =\", impurity)\n",
    "        else:\n",
    "            impurity = entropy(Y)\n",
    "            print(\"Current Entropy is =\", impurity) \n",
    "        \n",
    "        \n",
    "        # edge case 2 and 3 - (If we have run out of features to split upon) or (max depth limit reached)\n",
    "        if len(features) == 0 or level >= self.depth_limit:        \n",
    "            print(\"Reached leaf Node\\n\")\n",
    "            return Node(class_mark=get_most_occurring_feature(Y), entropy=impurity)\n",
    "    \n",
    "        # intermediate node\n",
    "        max_gain = -1\n",
    "        best_feature = -1 \n",
    "        best_x_plit = None\n",
    "        best_y_split = None\n",
    "        best_split_val = -1\n",
    "        X = np.array(X) # for ease of subscripting\n",
    "        # brute force search for best splitting feature\n",
    "        for feature in features:\n",
    "            x_split, y_split, gain, split_val = best_split(X, Y, feature, self.gini)              \n",
    "            if gain>max_gain:\n",
    "                max_gain = gain\n",
    "                best_feature = feature\n",
    "                best_y_split = y_split\n",
    "                best_x_split = x_split\n",
    "                best_split_val = split_val\n",
    "                \n",
    "        # creating the node with best feature and corresponding split value\n",
    "        node = Node(best_feature, best_split_val, entropy=impurity)\n",
    "        \n",
    "        # last line to print for an intermediate node\n",
    "        if self.gini:\n",
    "            print(\"Splitting on feature\",self.feature_names[best_feature] ,\"with gini_gain\", max_gain,\"\\n\")\n",
    "        else:\n",
    "            print(\"Splitting on feature\",self.feature_names[best_feature] ,\"with gain ratio\", max_gain,\"\\n\")\n",
    "        \n",
    "        \"\"\" features.remove(best_feature) \"\"\" \n",
    "        # commented out the code for removing feature for better accuracy\n",
    "        \n",
    "        # adding left and right child and creating the tree recursively\n",
    "        node.left = self.generate(best_x_split[0], best_y_split[0], features, level+1)\n",
    "        node.right = self.generate(best_x_split[1], best_y_split[1], features, level+1)\n",
    "        \n",
    "        return node\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        \"\"\" X_test : list(list(int)) \n",
    "            y_pred : prediction based on row-wise X_test data\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        for X_i in X_test:\n",
    "            y_pred.append(self.root.decide(X_i))\n",
    "            \n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    \n",
    "    def confusion_matrix(self, y_pred, y_true):\n",
    "        \"\"\" y_pred : list(int) representing prediction classes\n",
    "            y_true : list(int) representing true classes\n",
    "            both contains values from 0 to n-1\n",
    "        \"\"\"\n",
    "        n = len(self.classes)\n",
    "        j = 0\n",
    "        mat = np.array([0]*(n*n)).reshape(n,n) # -------------------------------> confusion matrix \n",
    "        \n",
    "        for k in range(len(y_true)):\n",
    "            i = self.classes.index(y_true[k]) #---------------------------------> row represents true class\n",
    "            j = y_pred[k] #-----------------------------------------------------> col represents prediction class\n",
    "            mat[i][j] += 1\n",
    "\n",
    "        return np.array(mat)\n",
    "    \n",
    "    \n",
    "    def node_string(self, node):\n",
    "        \"\"\" returns a string which represents the node in a dot graphviz format\n",
    "        \"\"\"\n",
    "        split_feature = None\n",
    "        if node.deciding_feature is not None:\n",
    "            split_feature = self.feature_names[node.deciding_feature]\n",
    "        class_name = \"no output\"\n",
    "        impurity = round(node.entropy,6)\n",
    "        if node.class_mark is not None:\n",
    "            class_name = self.classes[node.class_mark]\n",
    "        return \"\\n{} [label=\\\"Split feature : {}\\\\nNode impurity : {}\\\\nNode output : {}\\\" ];\".format(id(node),split_feature,impurity,class_name)\n",
    "                \n",
    "            \n",
    "    def to_pdf(self,filename=None):\n",
    "        \"\"\" returns the tree as dot data after saving it as a pdf\n",
    "        \"\"\"\n",
    "        dot_data = '''digraph Tree {\n",
    "        node [shape=box] ;'''\n",
    "        \n",
    "        queue = deque() #--------------------------------> to perform level order traversal\n",
    "        \n",
    "        # adding the root\n",
    "        r = self.root\n",
    "        queue.append(r)\n",
    "        dot_data = dot_data + self.node_string(r)\n",
    "        \n",
    "        # Doing LEVEL ORDER traversal in the tree (using a queue)\n",
    "        while len(queue) != 0 :\n",
    "            node = queue.popleft()\n",
    "            parent_split_feature = None\n",
    "            if node.deciding_feature is not None:\n",
    "                parent_split_feature = self.feature_names[node.deciding_feature]\n",
    "            if node.left is not None:                \n",
    "                # Creating left child\n",
    "                dot_data = dot_data + self.node_string(node.left) \n",
    "                # Connecting parent node with left child\n",
    "                dot_data = dot_data + \"\\n{} -> {} [ label=\\\"{}<={}\\\"]; \".format(id(node),id(node.left), parent_split_feature, node.split_val)\n",
    "                # Adding left child node to queue\n",
    "                queue.append(node.left)\n",
    "                \n",
    "            if node.right is not None:                \n",
    "                # Creating right child \n",
    "                dot_data = dot_data + self.node_string(node.right) \n",
    "                # Connecting parent node with right child\n",
    "                dot_data = dot_data + \"\\n{} -> {} [ label=\\\"{}>{}\\\"]; \".format(id(node),id(node.right), parent_split_feature, node.split_val)\n",
    "                # Adding right child node to queue\n",
    "                queue.append(node.right)\n",
    "        \n",
    "        dot_data = dot_data + \"\\n}\"\n",
    "\n",
    "        if filename != None:    \n",
    "            graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "            print(graph)\n",
    "            graph.write_pdf(filename)    \n",
    "        \n",
    "        return dot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 0 = 1\n",
      "Count of 1 = 3\n",
      "Current Entropy is = 0.8112781244591328\n",
      "Splitting on feature X[0] with gain ratio 0.31127812445913283 \n",
      "\n",
      "Level 1\n",
      "Count of 0 = 1\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature X[1] with gain ratio 1.0 \n",
      "\n",
      "Level 2\n",
      "Count of 0 = 1\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 1 = 1\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 1 = 2\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example tree\n",
    "clf = DecisionTree()\n",
    "clf.fit([[0,0],[0,1],[1,0],[1,1]],[0,1,1,1]) # training data for OR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing on the training data for OR gate\n",
    "clf.confusion_matrix(clf.predict([[0,0],[0,1],[1,0],[1,1]]),[0,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data = datasets.load_iris()\n",
    "X = iris_data.data\n",
    "Y = iris_data.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 2 = 38\n",
      "Count of 1 = 36\n",
      "Count of 0 = 38\n",
      "Current Entropy is = 1.5844996446144277\n",
      "Splitting on feature petal length (cm) with gain ratio 1.0 \n",
      "\n",
      "Level 1\n",
      "Count of 0 = 38\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 2 = 38\n",
      "Count of 1 = 36\n",
      "Current Entropy is = 0.9994730201859836\n",
      "Splitting on feature petal length (cm) with gain ratio 0.7949533984153239 \n",
      "\n",
      "Level 2\n",
      "Count of 1 = 36\n",
      "Count of 2 = 3\n",
      "Current Entropy is = 0.39124356362925566\n",
      "Splitting on feature petal width (cm) with gain ratio 0.38328097226983665 \n",
      "\n",
      "Level 3\n",
      "Count of 1 = 33\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 1 = 3\n",
      "Count of 2 = 3\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature sepal width (cm) with gain ratio 1.0 \n",
      "\n",
      "Level 4\n",
      "Count of 2 = 3\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of 1 = 3\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 2 = 35\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf1 = DecisionTree() # clf1 : my model using info gain\n",
    "clf1.fit(x_train, y_train,feature_names=iris_data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = DecisionTreeClassifier() # standard model of sklearn\n",
    "clf2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing information gain model against standard model:\n",
      "\n",
      "difference:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0] \n",
      "\n",
      "my model:\n",
      " [[12  0  0]\n",
      " [ 0 10  4]\n",
      " [ 0  0 12]] \n",
      "\n",
      "standard model:\n",
      " [[12  0  0]\n",
      " [ 0 10  4]\n",
      " [ 0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing information gain model against standard model:\\n\")\n",
    "print(\"difference:\\n\", clf1.predict(x_test)-clf2.predict(x_test),\"\\n\")\n",
    "print(\"my model:\\n\", clf1.confusion_matrix(clf1.predict(x_test),y_test),\"\\n\")\n",
    "print(\"standard model:\\n\", confusion_matrix(y_test, clf2.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 2 = 38\n",
      "Count of 1 = 36\n",
      "Count of 0 = 38\n",
      "Current gini impurity is = 0.666454081632653\n",
      "Splitting on feature petal length (cm) with gini_gain 0.33633825151682284 \n",
      "\n",
      "Level 1\n",
      "Count of 0 = 38\n",
      "Current gini impurity is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 2 = 38\n",
      "Count of 1 = 36\n",
      "Current gini impurity is = 0.4996347699050402\n",
      "Splitting on feature petal length (cm) with gini_gain 0.42479069506096545 \n",
      "\n",
      "Level 2\n",
      "Count of 1 = 36\n",
      "Count of 2 = 3\n",
      "Current gini impurity is = 0.14201183431952646\n",
      "Splitting on feature petal width (cm) with gini_gain 0.06508875739644954 \n",
      "\n",
      "Level 3\n",
      "Count of 1 = 33\n",
      "Current gini impurity is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 1 = 3\n",
      "Count of 2 = 3\n",
      "Current gini impurity is = 0.5\n",
      "Splitting on feature sepal width (cm) with gini_gain 0.5 \n",
      "\n",
      "Level 4\n",
      "Count of 2 = 3\n",
      "Current gini impurity is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of 1 = 3\n",
      "Current gini impurity is =  0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 2 = 35\n",
      "Current gini impurity is =  0.0\n",
      "Reached leaf Node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using gini index as metric \n",
    "clf3 = DecisionTree()\n",
    "clf3.fit(x_train, y_train, feature_names=iris_data.feature_names, metric=\"gini_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gini index model against standard model:\n",
      "\n",
      "difference:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0] \n",
      "\n",
      "my model:\n",
      " [[12  0  0]\n",
      " [ 0 10  4]\n",
      " [ 0  0 12]] \n",
      "\n",
      "standard model:\n",
      " [[12  0  0]\n",
      " [ 0 10  4]\n",
      " [ 0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing gini index model against standard model:\\n\")\n",
    "print(\"difference:\\n\", clf3.predict(x_test)-clf2.predict(x_test),\"\\n\")\n",
    "print(\"my model:\\n\", clf3.confusion_matrix(clf3.predict(x_test),y_test),\"\\n\")\n",
    "print(\"standard model:\\n\", confusion_matrix(y_test, clf2.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pydotplus.graphviz.Dot object at 0x0000021F9E3D7D00>\n",
      "<pydotplus.graphviz.Dot object at 0x0000021F9E4684C0>\n",
      "<pydotplus.graphviz.Dot object at 0x0000021F9E507190>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'digraph Tree {\\n        node [shape=box] ;\\n2334821344352 [label=\"Split feature : petal length (cm)\\\\nNode impurity : 0.666454\\\\nNode output : no output\" ];\\n2334821385792 [label=\"Split feature : None\\\\nNode impurity : 0\\\\nNode output : 0\" ];\\n2334821344352 -> 2334821385792 [ label=\"petal length (cm)<=2.95\"]; \\n2334821387760 [label=\"Split feature : petal length (cm)\\\\nNode impurity : 0.499635\\\\nNode output : no output\" ];\\n2334821344352 -> 2334821387760 [ label=\"petal length (cm)>2.95\"]; \\n2334767985712 [label=\"Split feature : petal width (cm)\\\\nNode impurity : 0.142012\\\\nNode output : no output\" ];\\n2334821387760 -> 2334767985712 [ label=\"petal length (cm)<=4.800000000000001\"]; \\n2334821742096 [label=\"Split feature : None\\\\nNode impurity : 0\\\\nNode output : 2\" ];\\n2334821387760 -> 2334821742096 [ label=\"petal length (cm)>4.800000000000001\"]; \\n2334821737616 [label=\"Split feature : None\\\\nNode impurity : 0\\\\nNode output : 1\" ];\\n2334767985712 -> 2334821737616 [ label=\"petal width (cm)<=1.5\"]; \\n2334821740160 [label=\"Split feature : sepal width (cm)\\\\nNode impurity : 0.5\\\\nNode output : no output\" ];\\n2334767985712 -> 2334821740160 [ label=\"petal width (cm)>1.5\"]; \\n2334821746864 [label=\"Split feature : None\\\\nNode impurity : 0\\\\nNode output : 2\" ];\\n2334821740160 -> 2334821746864 [ label=\"sepal width (cm)<=3.0999999999999996\"]; \\n2334821748544 [label=\"Split feature : None\\\\nNode impurity : 0\\\\nNode output : 1\" ];\\n2334821740160 -> 2334821748544 [ label=\"sepal width (cm)>3.0999999999999996\"]; \\n}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PATH'] = os.environ['PATH']+';'+os.environ['CONDA_PREFIX']+r\"\\Library\\bin\\graphviz\"\n",
    "clf.to_pdf(\"OR.pdf\") # --------------> metric - information gain\n",
    "clf1.to_pdf(\"Iris_info_gain.pdf\") # -> metric - information gain\n",
    "clf3.to_pdf(\"Iris_gini_indx.pdf\") # -> metric - gini index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
